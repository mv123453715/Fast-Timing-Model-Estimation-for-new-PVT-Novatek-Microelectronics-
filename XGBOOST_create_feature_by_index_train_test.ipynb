{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_input( train_path,test_path ):\n",
    "    train = pd.read_csv(train_path)\n",
    "    #print(train.head())\n",
    "    print('df.shape :',train.shape)\n",
    "    test = pd.read_csv(test_path)\n",
    "    #print(test.head())\n",
    "    print('df.shape :',test.shape)\n",
    "    return train,test\n",
    "\n",
    "def spilt_data( train,test ):\n",
    "    value_columns = []\n",
    "    for i in range(1,50):\n",
    "        string = 'value_'+str(i)\n",
    "        value_columns.append(string)   \n",
    "\n",
    "    train_y = train[value_columns]\n",
    "    test_y = test[value_columns]\n",
    "\n",
    "    for col in value_columns:\n",
    "        train.pop(col)\n",
    "        test.pop(col)\n",
    "\n",
    "    train_x = train\n",
    "    test_x = test\n",
    "    \n",
    "    return train_y,test_y,train_x,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(outputfile,predlist):\n",
    "    np.savetxt(outputfile, predlist, delimiter=\",\")\n",
    "    \n",
    "def getscore(test_y,predict_test):\n",
    "    #compute Accuracy\n",
    "    tmp = abs( test_y - predict_test )/abs( test_y )\n",
    "    #print(tmp)\n",
    "    #print(type(tmp))\n",
    "    #print(\"-----------------------------------------------------------\")\n",
    "    tmp[tmp >1] =1\n",
    "    #print(tmp**2)\n",
    "    square = tmp**2\n",
    "    #print(square.shape[1])\n",
    "    squ_sum = square.sum()\n",
    "    #print(sum1)\n",
    "    n = square.shape[0] * square.shape[1]\n",
    "    #print(n)\n",
    "    score = 100 - 100 * (np.sqrt(squ_sum/n))\n",
    "    print(\"accuracy:\" + str(score))\n",
    "    \n",
    "    return score,squ_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# xgboost模型\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "def xgb_train( train_x,train_y,test_x,test_y,testout_path ,test_file_name):\n",
    "    xgb_len = len(test_y.T)\n",
    "    maxdepth = [30]\n",
    "    lr = [0.07]\n",
    "    nest = [150]\n",
    "\n",
    "    #train all case\n",
    "    max_score = 0\n",
    "    best_md = 0\n",
    "    best_l = 0\n",
    "    best_n = 0\n",
    "    for md in maxdepth:\n",
    "        for l in lr:\n",
    "            for n in nest:\n",
    "                predict_test = []\n",
    "                print( \"md:\" + str(md) )\n",
    "                print( \"lr:\" + str(l) )\n",
    "                print( \"nest:\" + str(n) )\n",
    "\n",
    "                for i in range(xgb_len):\n",
    "                    #print(\"value\" +  str(i) +\"  training\" )\n",
    "                    params = {\n",
    "                    \"max_depth\":int(md),\n",
    "                    \"learning_rate\":float(l),\n",
    "                    \"n_estimators\":int(n),\n",
    "                    \"booster\":\"gbtree\",\n",
    "                    #\"subsample\":0.9,\n",
    "                    #\"colsample_bytree\":0.8\n",
    "                    }\n",
    "\n",
    "                    clf = xgb.XGBRegressor(**params)\n",
    "                    clf.fit(train_x,train_y.T[i])\n",
    "                    predict_test.append(clf.predict(test_x))\n",
    "\n",
    "                predict_test = np.array(predict_test).T\n",
    "                #print(predict_test)\n",
    "                testoutput = testout_path + \"\\\\\"+ test_file_name +\"_md_\" + str(md)  + \"lr_\" + str(l)  + \"nset_\" + str(n) +'.csv' \n",
    "                errorout = testout_path + \"\\\\\"+ test_file_name +\"md_\" + str(md)  + \"lr_\" + str(l)  + \"nset_\" + str(n) +'_error.csv' \n",
    "                output(testoutput,(abs(predict_test)))\n",
    "                output(errorout,(abs(predict_test - test_y)/abs( test_y ))**2)\n",
    "\n",
    "                score,squ_sum = getscore(test_y,predict_test)\n",
    "                \"\"\"\n",
    "                if (score > max_score):\n",
    "                    max_score = score\n",
    "                    best_md = md\n",
    "                    best_l = l\n",
    "                    best_n = n\n",
    "                \"\"\"\n",
    "\n",
    "    #print(\"max_score:\" +str(max_score))\n",
    "    #print(\"best_md:\" +str(best_md))\n",
    "    #print(\"best_l:\" +str(best_l))\n",
    "    #print(\"best_n:\" +str(best_n))\n",
    "    return squ_sum\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ff1p1v125c.csv\n",
      "C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ff1p1v125c_beta_100.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_paths=[]\n",
    "test_paths=[]\n",
    "train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ff1p1v125c.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ff1p1vm40c.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ss0p9v125c.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ss0p9vm40c.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_tt1p0v25c.csv')\n",
    "\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\std\\csv\\lib1_ff0p99v125c_base_400.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\std\\csv\\lib1_ff0p99vm40c_base_400.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\std\\csv\\lib1_ss0p81v125c_base_400.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\std\\csv\\lib1_ss0p81vm40c_base_400.csv')\n",
    "# train_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\std\\csv\\lib1_tt0p9v25c_base_400.csv')\n",
    "\n",
    "test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ff1p1v125c_beta_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ff1p1vm40c_beta_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ss0p9v125c_beta_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ss0p9vm40c_beta_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_tt1p0v25c_beta_100.csv')\n",
    "\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\std\\csv\\lib1_ff0p99v125c_alpha_100.csv')\n",
    "# test_paths.append( r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\std\\csv\\lib1_ff0p99vm40c_alpha_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\std\\csv\\lib1_ss0p81v125c_alpha_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\std\\csv\\lib1_ss0p81vm40c_alpha_100.csv')\n",
    "# test_paths.append(r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\std\\csv\\lib1_tt0p9v25c_alpha_100.csv')\n",
    "\n",
    "for i in range(len(test_paths)):\n",
    "    print(train_paths[i])\n",
    "    print(test_paths[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_feature(data,feature):\n",
    "    data.pop(feature)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_file:C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\train\\boost\\csv\\train_ff1p1v125c.csv\n",
      "test_file:C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test\\boost\\csv\\lib1_ff1p1v125c_beta_100.csv\n",
      "df.shape : (18858, 74)\n",
      "df.shape : (5658, 74)\n",
      "cell_trasition_power:1\n",
      "[1, 1, 7, 3, 3, 7, 1, 7, 1, 1, 1, 1, 1, 1, 1, 3, 9, 3, 9, 1, 1, 3, 9, 3, 3, 7, 3, 7, 1, 3, 3, 3, 3, 3, 3, 3, 9, 1, 1, 3, 7, 1, 3, 3, 7, 3, 7, 7, 3, 3, 1, 1, 1, 1, 9, 3, 7, 9, 1, 7, 3, 1, 3, 1, 9, 3, 9, 9, 3, 3, 1, 1, 9, 3, 3, 9, 1, 3, 1, 1, 1, 1, 3, 3, 9, 3, 1, 1, 1, 1, 9, 3, 3, 1, 1, 3, 3, 9, 9, 1, 1, 1, 7, 1, 1, 9, 1, 3, 7, 9, 7, 3, 1, 9, 3, 3, 3, 3, 3, 1, 1, 9, 7, 3, 3, 3, 3, 3, 1, 9, 1, 1, 1, 1, 3, 9, 1, 9, 7, 9, 9, 1, 1, 3, 7, 3, 1, 3, 3, 1, 1, 1, 1, 3, 1, 7, 3, 1, 1, 9, 9, 3, 1, 3, 1, 9, 3, 3, 1, 3, 3, 1, 3, 7, 1, 1, 1, 3, 7, 1, 1, 9, 3, 1, 1, 3, 7, 1, 9, 9, 1, 3, 3, 1, 2, 1, 7, 1, 7, 3, 1, 1, 3, 1, 1, 3, 9, 9, 7, 9, 9, 2, 1, 3, 3, 3, 3, 3, 9, 9, 1, 3, 3, 2, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 7, 9, 1, 1, 9, 1, 1, 3, 3, 1, 3, 1, 1, 9, 9, 7, 9, 3, 8, 3, 9, 1, 3, 3, 3, 1, 1, 3, 1, 3, 4, 1, 1, 3, 3, 9, 3, 9, 1, 9, 1, 9, 3, 1, 1, 1, 7, 1, 9, 7, 3, 3, 1, 1, 1, 3, 3, 2, 9, 7, 1, 9, 3, 3, 3, 1, 3, 3, 7, 9, 1, 9, 1, 3, 1, 9, 3, 10, 3, 1, 3, 9, 3, 1, 1, 3, 2, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 2, 9, 3, 2, 1, 3, 1, 1, 2, 1, 3, 3, 3, 1, 3, 9, 6, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 9, 1, 1, 1, 2, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 9, 1, 2, 3, 1, 7, 1, 1, 9, 3, 3, 3, 1, 1, 1, 1, 1, 8, 3, 1, 1, 3, 1, 3, 1, 3, 1, 9, 3, 9, 1, 1, 1, 3, 1, 1, 1, 1, 9, 7, 1, 9, 3, 3, 3, 3, 1, 9, 1, 1, 3, 2, 1, 3, 1, 8, 9, 1, 3, 3, 1, 1, 3, 1, 2, 3, 3, 1, 1, 1, 1, 1, 6, 6, 4, 3, 1, 1, 3, 1, 1, 8, 3, 9, 8, 1, 1, 1, 1, 9, 8, 9, 6, 2, 1, 2, 2, 2, 8, 9, 1, 8, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 2, 3, 2, 2, 1, 2, 1, 2, 2, 3, 1, 4, 1, 8, 1, 9, 1, 1, 1, 3, 1, 3, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 4, 3, 1, 1, 4, 3, 3, 3, 8, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 8, 1, 6, 1, 3, 1, 1, 8, 9, 1, 1, 4, 1, 8, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 1, 3, 1, 1, 2, 2, 1, 1, 9, 3, 1, 1, 1, 1, 4, 2, 2, 1, 1, 1, 3, 1, 1, 1, 2, 3, 3, 3, 3, 1, 3, 3, 1, 3, 9, 9, 3, 3, 2, 1, 2, 2, 2, 3, 1, 3, 1, 1, 1, 2, 3, 1, 2, 2, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 9, 1, 3, 1, 1, 8, 3, 3, 1, 1, 2, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 2, 3, 2, 3, 3, 1, 2, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 2, 9, 3, 1, 3, 3, 1, 2, 2, 1, 1, 9, 1, 1, 9, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 2, 2, 1, 1, 1, 2, 8, 8, 1, 8, 1, 3, 1, 3, 3, 3, 3, 4, 1, 3, 2, 3, 2, 1, 1, 3, 2, 3, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 8, 1, 8, 1, 1, 1, 4, 1, 2, 4, 2, 1, 2, 1, 1, 1, 2, 2, 1, 16, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 4, 1, 1, 1, 4, 1, 1, 9, 1, 2, 4, 1, 1, 2, 3, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 2, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 4, 1, 3, 1, 8, 9, 3, 1, 3, 2, 3, 7, 2, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 1, 3, 2, 1, 1, 9, 1, 1, 1, 3, 3, 1, 3, 1, 3, 9, 3, 9, 7, 3, 3, 4, 1, 1, 3, 1, 8, 1, 8, 9, 8, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 7, 1, 4, 1, 3, 1, 1, 1, 3, 1, 4, 2, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 7, 3, 3, 2, 1, 3, 2, 2, 2, 1, 1, 1, 1, 1, 3, 1, 1, 2, 7, 3, 3, 1, 2, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3, 7, 3, 3, 1, 1, 3, 4, 16, 3, 3, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 1, 16, 1, 2, 16, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 7, 1, 1, 1, 1, 3, 7, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 7, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "       cell_trasition_power  capacitance_cnt_index  capacitance  index2_2  \\\n",
      "10700                     1                      1     0.000472  0.001005   \n",
      "10704                     1                      1     0.000473  0.001005   \n",
      "7114                      1                      1     0.000482  0.001017   \n",
      "7110                      1                      2     0.000482  0.001017   \n",
      "7106                      1                      3     0.000482  0.001017   \n",
      "7090                      1                      4     0.000482  0.001017   \n",
      "7094                      1                      5     0.000482  0.001017   \n",
      "7098                      1                      6     0.000482  0.001017   \n",
      "7102                      1                      7     0.000482  0.001017   \n",
      "12808                     1                      1     0.000488  0.001018   \n",
      "12816                     1                      2     0.000488  0.001018   \n",
      "12812                     1                      3     0.000488  0.001018   \n",
      "12820                     1                      1     0.000493  0.001018   \n",
      "12824                     1                      2     0.000493  0.001018   \n",
      "12828                     1                      3     0.000493  0.001018   \n",
      "7118                      1                      1     0.000499  0.001017   \n",
      "7122                      1                      2     0.000499  0.001017   \n",
      "7126                      1                      3     0.000499  0.001017   \n",
      "7134                      1                      4     0.000499  0.001017   \n",
      "7138                      1                      5     0.000499  0.001017   \n",
      "7130                      1                      6     0.000499  0.001017   \n",
      "7142                      1                      7     0.000499  0.001017   \n",
      "102                       1                      1     0.000500  0.001211   \n",
      "7066                      1                      1     0.000510  0.001017   \n",
      "7082                      1                      2     0.000510  0.001017   \n",
      "7078                      1                      3     0.000510  0.001017   \n",
      "7074                      1                      4     0.000510  0.001017   \n",
      "7070                      1                      5     0.000510  0.001017   \n",
      "7062                      1                      6     0.000510  0.001017   \n",
      "7086                      1                      7     0.000510  0.001017   \n",
      "...                     ...                    ...          ...       ...   \n",
      "6044                      1                      2     0.005392  0.001892   \n",
      "6036                      1                      3     0.005392  0.001892   \n",
      "6068                      1                      1     0.005510  0.001892   \n",
      "6064                      1                      2     0.005510  0.001892   \n",
      "6060                      1                      3     0.005510  0.001892   \n",
      "7440                      1                      1     0.005589  0.005269   \n",
      "16784                     1                      1     0.005599  0.001873   \n",
      "16792                     1                      2     0.005599  0.001873   \n",
      "16788                     1                      3     0.005599  0.001873   \n",
      "16760                     1                      1     0.005739  0.001873   \n",
      "16764                     1                      2     0.005739  0.001873   \n",
      "16768                     1                      3     0.005739  0.001873   \n",
      "7302                      1                      1     0.005774  0.006237   \n",
      "7308                      1                      1     0.006636  0.007253   \n",
      "7868                      1                      1     0.006930  0.003357   \n",
      "7470                      1                      1     0.007387  0.003892   \n",
      "7880                      1                      1     0.007655  0.003616   \n",
      "7476                      1                      1     0.008210  0.004062   \n",
      "7886                      1                      1     0.008388  0.003885   \n",
      "7482                      1                      1     0.008901  0.004308   \n",
      "7892                      1                      1     0.009202  0.004166   \n",
      "7898                      1                      1     0.010011  0.004436   \n",
      "7494                      1                      1     0.010897  0.005128   \n",
      "7500                      1                      1     0.012265  0.005626   \n",
      "7910                      1                      1     0.012363  0.005242   \n",
      "7512                      1                      1     0.013756  0.006185   \n",
      "7916                      1                      1     0.013948  0.005766   \n",
      "7928                      1                      1     0.015515  0.006313   \n",
      "7518                      1                      1     0.016396  0.007256   \n",
      "7934                      1                      1     0.018615  0.007353   \n",
      "\n",
      "       index2_3  index2_4  index2_5  index2_6  index2_7   value_1  ...  \\\n",
      "10700  0.001020  0.001051  0.001098  0.001164  0.001251  0.034036  ...   \n",
      "10704  0.001020  0.001051  0.001098  0.001164  0.001251  0.035971  ...   \n",
      "7114   0.001076  0.001190  0.001366  0.001614  0.001941  0.053672  ...   \n",
      "7110   0.001076  0.001190  0.001366  0.001614  0.001941  0.057757  ...   \n",
      "7106   0.001076  0.001190  0.001366  0.001614  0.001941  0.058140  ...   \n",
      "7090   0.001076  0.001190  0.001366  0.001614  0.001941  0.082063  ...   \n",
      "7094   0.001076  0.001190  0.001366  0.001614  0.001941  0.076070  ...   \n",
      "7098   0.001076  0.001190  0.001366  0.001614  0.001941  0.062006  ...   \n",
      "7102   0.001076  0.001190  0.001366  0.001614  0.001941  0.071086  ...   \n",
      "12808  0.001079  0.001197  0.001380  0.001637  0.001975  0.031913  ...   \n",
      "12816  0.001079  0.001197  0.001380  0.001637  0.001975  0.031947  ...   \n",
      "12812  0.001079  0.001197  0.001380  0.001637  0.001975  0.032778  ...   \n",
      "12820  0.001079  0.001197  0.001380  0.001637  0.001975  0.033081  ...   \n",
      "12824  0.001079  0.001197  0.001380  0.001637  0.001975  0.033927  ...   \n",
      "12828  0.001079  0.001197  0.001380  0.001637  0.001975  0.033082  ...   \n",
      "7118   0.001076  0.001190  0.001366  0.001614  0.001941  0.087177  ...   \n",
      "7122   0.001076  0.001190  0.001366  0.001614  0.001941  0.080954  ...   \n",
      "7126   0.001076  0.001190  0.001366  0.001614  0.001941  0.065850  ...   \n",
      "7134   0.001076  0.001190  0.001366  0.001614  0.001941  0.061910  ...   \n",
      "7138   0.001076  0.001190  0.001366  0.001614  0.001941  0.061503  ...   \n",
      "7130   0.001076  0.001190  0.001366  0.001614  0.001941  0.075913  ...   \n",
      "7142   0.001076  0.001190  0.001366  0.001614  0.001941  0.057092  ...   \n",
      "102    0.001952  0.003367  0.005571  0.008663  0.012733  0.070264  ...   \n",
      "7066   0.001076  0.001190  0.001366  0.001614  0.001941  0.071812  ...   \n",
      "7082   0.001076  0.001190  0.001366  0.001614  0.001941  0.054402  ...   \n",
      "7078   0.001076  0.001190  0.001366  0.001614  0.001941  0.054794  ...   \n",
      "7074   0.001076  0.001190  0.001366  0.001614  0.001941  0.066754  ...   \n",
      "7070   0.001076  0.001190  0.001366  0.001614  0.001941  0.058689  ...   \n",
      "7062   0.001076  0.001190  0.001366  0.001614  0.001941  0.077635  ...   \n",
      "7086   0.001076  0.001190  0.001366  0.001614  0.001941  0.050644  ...   \n",
      "...         ...       ...       ...       ...       ...       ...  ...   \n",
      "6044   0.005024  0.011005  0.020322  0.033396  0.050599  0.014999  ...   \n",
      "6036   0.005024  0.011005  0.020322  0.033396  0.050599  0.022679  ...   \n",
      "6068   0.005024  0.011005  0.020322  0.033396  0.050599  0.022618  ...   \n",
      "6064   0.005024  0.011005  0.020322  0.033396  0.050599  0.026392  ...   \n",
      "6060   0.005024  0.011005  0.020322  0.033396  0.050599  0.029708  ...   \n",
      "7440   0.020253  0.048867  0.093446  0.155996  0.238304  0.030331  ...   \n",
      "16784  0.004937  0.010787  0.019903  0.032692  0.049522  0.030392  ...   \n",
      "16792  0.004937  0.010787  0.019903  0.032692  0.049522  0.028024  ...   \n",
      "16788  0.004937  0.010787  0.019903  0.032692  0.049522  0.029250  ...   \n",
      "16760  0.004937  0.010787  0.019903  0.032692  0.049522  0.020923  ...   \n",
      "16764  0.004937  0.010787  0.019903  0.032692  0.049522  0.021899  ...   \n",
      "16768  0.004937  0.010787  0.019903  0.032692  0.049522  0.020963  ...   \n",
      "7302   0.024620  0.059724  0.114416  0.191154  0.292131  0.023842  ...   \n",
      "7308   0.029202  0.071116  0.136417  0.228042  0.348608  0.024226  ...   \n",
      "7868   0.011630  0.027427  0.052040  0.086574  0.132016  0.006663  ...   \n",
      "7470   0.014044  0.033431  0.063635  0.106014  0.161780  0.005671  ...   \n",
      "7880   0.012800  0.030336  0.057658  0.095993  0.146437  0.006406  ...   \n",
      "7476   0.014810  0.035334  0.067310  0.112176  0.171213  0.005749  ...   \n",
      "7886   0.014014  0.033356  0.063490  0.105772  0.161409  0.006438  ...   \n",
      "7482   0.015920  0.038094  0.072640  0.121112  0.184895  0.005672  ...   \n",
      "7892   0.015279  0.036501  0.069564  0.115955  0.177000  0.006283  ...   \n",
      "7898   0.016495  0.039525  0.075405  0.125748  0.191992  0.006372  ...   \n",
      "7494   0.019620  0.047294  0.090409  0.150904  0.230508  0.005492  ...   \n",
      "7500   0.021867  0.052879  0.101195  0.168988  0.258195  0.005587  ...   \n",
      "7910   0.020132  0.048567  0.092868  0.155027  0.236819  0.006169  ...   \n",
      "7512   0.024385  0.059140  0.113286  0.189260  0.289232  0.005607  ...   \n",
      "7916   0.022497  0.054446  0.104221  0.174061  0.265961  0.006218  ...   \n",
      "7928   0.024965  0.060583  0.116073  0.193932  0.296385  0.006204  ...   \n",
      "7518   0.029217  0.071154  0.136490  0.228164  0.348795  0.005530  ...   \n",
      "7934   0.029654  0.072241  0.138588  0.231682  0.354181  0.006223  ...   \n",
      "\n",
      "       value_41  value_42  value_43  value_44  value_45  value_46  value_47  \\\n",
      "10700  0.132399  0.135264  0.152603  0.152787  0.153430  0.154652  0.156541   \n",
      "10704  0.138211  0.141282  0.160918  0.161091  0.162735  0.163883  0.165489   \n",
      "7114   0.163873  0.173397  0.171355  0.172005  0.174272  0.178384  0.184904   \n",
      "7110   0.167052  0.177261  0.174442  0.175113  0.177443  0.181835  0.188589   \n",
      "7106   0.167384  0.177604  0.174814  0.175488  0.177826  0.182234  0.189013   \n",
      "7090   0.193055  0.203968  0.201505  0.202188  0.204575  0.209089  0.216397   \n",
      "7094   0.185208  0.196114  0.192680  0.193370  0.195766  0.200324  0.207305   \n",
      "7098   0.173751  0.183469  0.182523  0.183176  0.185200  0.189455  0.195947   \n",
      "7102   0.178578  0.189695  0.184830  0.185556  0.188090  0.192879  0.199578   \n",
      "12808  0.146833  0.157112  0.152170  0.152935  0.156047  0.160936  0.168814   \n",
      "12816  0.145399  0.155684  0.149605  0.150301  0.153224  0.157640  0.165892   \n",
      "12812  0.148888  0.159124  0.155613  0.156148  0.158329  0.163417  0.171421   \n",
      "12820  0.148951  0.158968  0.157730  0.158436  0.160966  0.165537  0.172735   \n",
      "12824  0.151056  0.161408  0.160198  0.160899  0.163400  0.167955  0.175093   \n",
      "12828  0.147010  0.158284  0.154527  0.155249  0.157834  0.162150  0.169232   \n",
      "7118   0.203626  0.214216  0.216300  0.216881  0.219265  0.223727  0.230486   \n",
      "7122   0.196028  0.206585  0.207454  0.208137  0.210524  0.215038  0.221863   \n",
      "7126   0.184175  0.193893  0.196563  0.197200  0.199424  0.203629  0.210058   \n",
      "7134   0.178074  0.188049  0.189111  0.189763  0.192040  0.196332  0.202906   \n",
      "7138   0.177679  0.187609  0.188730  0.189381  0.191651  0.195932  0.202484   \n",
      "7130   0.189467  0.200232  0.199481  0.200179  0.202617  0.207226  0.214288   \n",
      "7142   0.174167  0.183634  0.185565  0.186198  0.188405  0.192568  0.198925   \n",
      "102    0.194991  0.225054  0.135996  0.138922  0.148596  0.163942  0.184154   \n",
      "7066   0.177111  0.188285  0.181363  0.182091  0.184630  0.189458  0.196850   \n",
      "7082   0.159653  0.169795  0.162617  0.163323  0.165783  0.170665  0.177575   \n",
      "7078   0.159876  0.170164  0.162954  0.163661  0.166453  0.171029  0.178059   \n",
      "7074   0.170158  0.181442  0.172651  0.173398  0.176008  0.180963  0.188552   \n",
      "7070   0.166236  0.176224  0.171295  0.171977  0.174362  0.178850  0.185644   \n",
      "7062   0.184787  0.195841  0.190486  0.191206  0.193737  0.198507  0.205857   \n",
      "7086   0.156418  0.166281  0.159771  0.160464  0.162920  0.167071  0.173890   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "6044   0.146747  0.181356  0.070713  0.074501  0.087376  0.108352  0.137064   \n",
      "6036   0.176288  0.215508  0.098066  0.102029  0.115377  0.138071  0.169141   \n",
      "6068   0.149112  0.182790  0.074869  0.078519  0.090563  0.110830  0.138503   \n",
      "6064   0.158250  0.195066  0.078972  0.082819  0.095499  0.116929  0.146298   \n",
      "6060   0.163134  0.199563  0.087777  0.091394  0.103359  0.123940  0.152583   \n",
      "7440   0.165086  0.189331  0.122766  0.126055  0.135975  0.150821  0.168967   \n",
      "16784  0.163975  0.199178  0.097293  0.100635  0.111733  0.130948  0.157817   \n",
      "16792  0.160386  0.195740  0.089750  0.093350  0.104898  0.124841  0.152401   \n",
      "16788  0.163359  0.198644  0.096255  0.099718  0.110640  0.129924  0.156855   \n",
      "16760  0.149965  0.186499  0.069801  0.073720  0.086877  0.108830  0.138552   \n",
      "16764  0.151858  0.188453  0.073525  0.077273  0.089786  0.111138  0.140395   \n",
      "16768  0.148375  0.185198  0.064756  0.068936  0.082465  0.104975  0.135235   \n",
      "7302   0.156319  0.190900  0.090052  0.093854  0.105235  0.122321  0.143878   \n",
      "7308   0.157524  0.192208  0.091475  0.095308  0.106656  0.123717  0.145295   \n",
      "7868   0.184386  0.236074  0.037222  0.047041  0.074409  0.113322  0.161217   \n",
      "7470   0.179369  0.233359  0.015986  0.026808  0.056922  0.099629  0.149976   \n",
      "7880   0.184547  0.237294  0.037200  0.047090  0.074151  0.113696  0.162160   \n",
      "7476   0.176190  0.228435  0.015727  0.026295  0.055600  0.097509  0.146317   \n",
      "7886   0.184706  0.236677  0.037109  0.046968  0.074569  0.113591  0.161904   \n",
      "7482   0.175567  0.227592  0.015927  0.026297  0.055737  0.096955  0.145898   \n",
      "7892   0.184792  0.236984  0.035917  0.045999  0.073804  0.113002  0.161351   \n",
      "7898   0.185044  0.237121  0.036075  0.046051  0.073638  0.112913  0.161514   \n",
      "7494   0.176957  0.229102  0.014541  0.025396  0.055601  0.097561  0.147289   \n",
      "7500   0.176709  0.229289  0.013951  0.024963  0.055125  0.096946  0.146779   \n",
      "7910   0.184982  0.237192  0.035353  0.045536  0.073563  0.112848  0.161279   \n",
      "7512   0.177270  0.229871  0.014918  0.025916  0.055978  0.097990  0.148065   \n",
      "7916   0.184816  0.237014  0.035885  0.045950  0.073847  0.113188  0.161886   \n",
      "7928   0.184872  0.237529  0.035355  0.045475  0.073457  0.113016  0.161805   \n",
      "7518   0.178518  0.231601  0.015103  0.026289  0.056684  0.098856  0.149428   \n",
      "7934   0.185110  0.237541  0.035901  0.045990  0.073972  0.113863  0.162200   \n",
      "\n",
      "       value_48  value_49  capacitance_total_cnt  \n",
      "10700  0.159159  0.163415                      1  \n",
      "10704  0.167142  0.170376                      1  \n",
      "7114   0.193423  0.204633                      7  \n",
      "7110   0.197419  0.208561                      7  \n",
      "7106   0.197841  0.209011                      7  \n",
      "7090   0.225440  0.237689                      7  \n",
      "7094   0.216860  0.229038                      7  \n",
      "7098   0.204467  0.215787                      7  \n",
      "7102   0.209720  0.221772                      7  \n",
      "12808  0.178726  0.190441                      3  \n",
      "12816  0.175725  0.187763                      3  \n",
      "12812  0.180806  0.192949                      3  \n",
      "12820  0.181898  0.194441                      3  \n",
      "12824  0.183852  0.195439                      3  \n",
      "12828  0.178919  0.190661                      3  \n",
      "7118   0.239766  0.252457                      7  \n",
      "7122   0.231225  0.243578                      7  \n",
      "7126   0.218947  0.230379                      7  \n",
      "7134   0.211890  0.223432                      7  \n",
      "7138   0.211429  0.222826                      7  \n",
      "7130   0.223830  0.236028                      7  \n",
      "7142   0.207600  0.218608                      7  \n",
      "102    0.209252  0.239936                      1  \n",
      "7066   0.206118  0.219302                      7  \n",
      "7082   0.186612  0.198579                      7  \n",
      "7078   0.187013  0.198977                      7  \n",
      "7074   0.198235  0.211662                      7  \n",
      "7070   0.194312  0.206359                      7  \n",
      "7062   0.214922  0.227947                      7  \n",
      "7086   0.183252  0.194917                      7  \n",
      "...         ...       ...                    ...  \n",
      "6044   0.172366  0.212806                      3  \n",
      "6036   0.207978  0.252817                      3  \n",
      "6068   0.172602  0.211817                      3  \n",
      "6064   0.182278  0.224237                      3  \n",
      "6060   0.187865  0.229322                      3  \n",
      "7440   0.190105  0.215147                      1  \n",
      "16784  0.191490  0.231456                      3  \n",
      "16792  0.186827  0.227072                      3  \n",
      "16788  0.190582  0.230424                      3  \n",
      "16760  0.174351  0.216378                      3  \n",
      "16764  0.176222  0.217894                      3  \n",
      "16768  0.171616  0.214169                      3  \n",
      "7302   0.171004  0.205872                      1  \n",
      "7308   0.172439  0.207279                      1  \n",
      "7868   0.216831  0.278412                      1  \n",
      "7470   0.208209  0.271280                      1  \n",
      "7880   0.217016  0.279186                      1  \n",
      "7476   0.203991  0.265871                      1  \n",
      "7886   0.216465  0.277905                      1  \n",
      "7482   0.203284  0.265650                      1  \n",
      "7892   0.216718  0.278094                      1  \n",
      "7898   0.217063  0.278091                      1  \n",
      "7494   0.204326  0.266984                      1  \n",
      "7500   0.203583  0.266498                      1  \n",
      "7910   0.216838  0.278378                      1  \n",
      "7512   0.205214  0.267890                      1  \n",
      "7916   0.217548  0.279111                      1  \n",
      "7928   0.217383  0.278925                      1  \n",
      "7518   0.206702  0.269954                      1  \n",
      "7934   0.218042  0.279887                      1  \n",
      "\n",
      "[3140 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "squ_sum = 0.0\n",
    "n = 0\n",
    "for i in range(len(test_paths)):\n",
    "    print( \"train_file:\" + train_paths[i] )\n",
    "    print( \"test_file:\" + test_paths[i] )\n",
    "    train,test = read_input( train_paths[i],test_paths[i] )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #create new feature\n",
    "    train.insert(1,'cell_trasition_power',train['cell_rise']*1+train['rise_transition']*2\n",
    "                                            +train['cell_fall']*3+train['fall_transition']*4\n",
    "                                            +train['rise_power']*5+train['fall_power']*6)\n",
    "    test.insert(1,'cell_trasition_power',test['cell_rise']*1+test['rise_transition']*2\n",
    "                                            +test['cell_fall']*3+test['fall_transition']*4\n",
    "                                            +test['rise_power']*5+test['fall_power']*6)\n",
    "\n",
    "    #delete minor feature\n",
    "    train = delete_feature(train,\"temperature\")\n",
    "    train = delete_feature(train,\"voltage\")\n",
    "    train = delete_feature(train,\"cell_rise\")\n",
    "    train = delete_feature(train,\"rise_transition\")\n",
    "    train = delete_feature(train,\"cell_fall\")\n",
    "    train = delete_feature(train,\"fall_transition\")\n",
    "    train = delete_feature(train,\"rise_power\")\n",
    "    train = delete_feature(train,\"fall_power\")\n",
    "    train = delete_feature(train,\"index1_1\")\n",
    "    train = delete_feature(train,\"index1_2\")\n",
    "    train = delete_feature(train,\"index1_3\")\n",
    "    train = delete_feature(train,\"index1_4\")\n",
    "    train = delete_feature(train,\"index1_5\")\n",
    "    train = delete_feature(train,\"index1_6\")\n",
    "    train = delete_feature(train,\"index1_7\")\n",
    "    train = delete_feature(train,\"index2_1\")\n",
    "    train = delete_feature(train,\"rise_capacitance\")\n",
    "    train = delete_feature(train,\"fall_capacitance\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    test = delete_feature(test,\"temperature\")\n",
    "    test = delete_feature(test,\"voltage\")\n",
    "    test = delete_feature(test,\"cell_rise\")\n",
    "    test = delete_feature(test,\"rise_transition\")\n",
    "    test = delete_feature(test,\"cell_fall\")\n",
    "    test = delete_feature(test,\"fall_transition\")\n",
    "    test = delete_feature(test,\"rise_power\")\n",
    "    test = delete_feature(test,\"fall_power\")    \n",
    "    test = delete_feature(test,\"index1_1\")\n",
    "    test = delete_feature(test,\"index1_2\")\n",
    "    test = delete_feature(test,\"index1_3\")\n",
    "    test = delete_feature(test,\"index1_4\")\n",
    "    test = delete_feature(test,\"index1_5\")\n",
    "    test = delete_feature(test,\"index1_6\")\n",
    "    test = delete_feature(test,\"index1_7\")\n",
    "    test = delete_feature(test,\"index2_1\")\n",
    "    test = delete_feature(test,\"rise_capacitance\")\n",
    "    test = delete_feature(test,\"fall_capacitance\")\n",
    "    \n",
    "    #print(train)\n",
    "    #print(test)\n",
    "    \n",
    "    #df.groupby(['id'],as_index=False)['id'].agg({'cnt':'count'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    for ctp in range(1,2):   \n",
    "        print(\"cell_trasition_power:\" + str(ctp))\n",
    "        #train_x=train_x.sort_values([\"cell_trasition_power\"])\n",
    "        train_1 = train[train['cell_trasition_power']==ctp]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #test_x=test_x.sort_values([\"cell_trasition_power\"])    \n",
    "        test_1 = test[test['cell_trasition_power']==ctp]\n",
    "        \n",
    "        #train_1 = train_1.drop_duplicates(subset=['capacitance'],keep='first',inplace=False)\n",
    "        #test_1 = test_1.drop_duplicates(subset=['capacitance'],keep='first',inplace=False)\n",
    "        \n",
    "        train_1=train_1.sort_values([\"capacitance\"])\n",
    "        train_1_cnt =  train_1['capacitance'].value_counts(sort=False) \n",
    "        \n",
    "        train_1_cnt = train_1_cnt.sort_index()\n",
    "        train_1_cnt_dict = train_1_cnt.to_dict()\n",
    "        train_1['capacitance_total_cnt']= train_1['capacitance'].map(train_1_cnt_dict)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        train_1_cnt_list = train_1_cnt.to_list()\n",
    "        print(train_1_cnt_list)\n",
    "        train_1_cnt_list_insert = []\n",
    "        for cnts in train_1_cnt_list:\n",
    "            for cnt in range(cnts):\n",
    "                train_1_cnt_list_insert.append(cnt+1)\n",
    "                \n",
    "        #print(train_1_cnt_list_insert)\n",
    "        train_1.insert(1,'capacitance_cnt_index',train_1_cnt_list_insert)                \n",
    "        print(train_1)\n",
    "\n",
    "        \n",
    "    \n",
    "#         train_y,test_y,train_x,test_x = spilt_data( train_1,test_1 )\n",
    "    \n",
    "#         n = n + test_y.shape[0] * test_y.shape[1]\n",
    "#         print(\"n:\" + str(n))\n",
    "#         #dataframe -> numpy.array.value\n",
    "#         train_x = train_x.values\n",
    "#         train_y = train_y.values\n",
    "#         test_x = test_x.values\n",
    "#         test_y = test_y.values\n",
    "#         testout_path = r'C:\\Users\\User\\Fast_Timing_Model_Estimation_for_new_PVT-master\\test_record'\n",
    "#         file,filesplit = os.path.split(test_paths[i])\n",
    "#         test_file_name = filesplit.split('.')[0]\n",
    "#         #print(test_file_name)\n",
    "#         squ_sum = squ_sum+xgb_train( train_x,train_y,test_x,test_y,testout_path ,test_file_name)\n",
    "\n",
    "\n",
    "# score = 100 - 100 * (np.sqrt(squ_sum/n))\n",
    "# print(\"total_score:\" + str(score))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
